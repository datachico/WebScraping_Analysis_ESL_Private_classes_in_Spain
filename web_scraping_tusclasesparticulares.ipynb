{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "import time\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.parser import parse as parse_date\n",
    "import pdb\n",
    "import pytz\n",
    "utc=pytz.UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTION PLAN\n",
    "\n",
    "1)  Start with region link.  Get all regional capitals.  Examples:\n",
    "https://www.tusclasesparticulares.com/profesores-ingles/madrid-capital.aspx\n",
    "https://www.tusclasesparticulares.com/profesores-ingles/bilbao\n",
    "\n",
    "2) GET REGION URLS.  From base URL use \"get_region_urls\" to get the individual urls of all pages for specific region.\n",
    "\n",
    "3) GET PROFILE URLS FROM RESULTS PAGE ON EACH REGION URL.  Use get_profile_urls.  Input is region URL.  Output is dictionary with profiles URL, picture boolean, profile type.  \n",
    "\n",
    "4) SCRAPE INDIVIDUAL PROFILE URLS.  Input is dictionary of url of profile (+ extra information).  Output is dataframe.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_region(base_region_url, region):\n",
    "    \n",
    "    #2) GET REGION URLS.  l\n",
    "    region_urls = get_region_urls(base_region_url)\n",
    "    #list of region urls\n",
    "    \n",
    "    #3) GET PROFILE URLS FROM RESULTS PAGE ON EACH REGION URL.\n",
    "    profile_dicts = []\n",
    "    for results_page in region_urls:\n",
    "        #get list of profile dictionaries for each results page, \n",
    "        #then concatenate so you have big list of dictionaries that you can turn into a DF\n",
    "        profile_dicts.extend(get_profile_urls(results_page))\n",
    "        \n",
    "        #now you have list of profile dictionaries for ALL results page for region\n",
    "    \n",
    "    #4) SCRAPE INDIVIDUAL PROFILE URLS.  \n",
    "    #input is list of ALL profile dicts for region.  Output should be oe dataframe that has all region data\n",
    "    print('length of profile_dicts:')\n",
    "    print(len(profile_dicts))\n",
    "    list_of_scrapped_profiles = []\n",
    "    for profile_dictionary in profile_dicts:\n",
    "        print('scraping this profile URL:')\n",
    "        print(profile_dictionary['profile_url'])\n",
    "        print('from this url results page:')\n",
    "        print(profile_dictionary['results_page_url'])\n",
    "        print('---------------')\n",
    "        \n",
    "        #scrape profile URL and return dictionary with ALL info for profiles\n",
    "        scrapped_profile = profile_extraction(profile_dictionary)\n",
    "    \n",
    "        #add this dictionary to list\n",
    "        list_of_scrapped_profiles.append(scrapped_profile)\n",
    "    pdb.set_trace()\n",
    "    #turn list of dicts into dataframe, add date\n",
    "    df = pd.DataFrame(list_of_scrapped_profiles)\n",
    "    df['date_scraped'] = datetime.now(pytz.utc)\n",
    "    df['base_region'] = region\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INPUT - list of dicts with profile URL (and profile_type, picture info, page rank)\n",
    "#MIDDLE - calls scrapping to get dictionary, appends to list\n",
    "#RETURN - dataframe of all scrapped urls\n",
    "\n",
    "#example of input:\n",
    "# {'profile_url': 'https://www.tusclasesparticulares.com/profesores/vizcaya/clases-ingles-ninos-adultos-bilbao-1932040',\n",
    "#   'page_rank': 1,\n",
    "#   'profe_status': 'trr aitem int',\n",
    "#   'has_picture': True}\n",
    "\n",
    "def scrape_profiles_by_region(profile_dicts):\n",
    "    list_of_scrapped_profiles = []\n",
    "    for profile_dictionary in profile_dicts:\n",
    "        scrapped_profile = profile_extraction(profile_dictionary)\n",
    "        list_of_scrapped_profiles.append(scrapped_profile)\n",
    "        print('scraping this profile URL:')\n",
    "        print(profile_dictionary['profile_url'])\n",
    "        print('from this url results page:')\n",
    "        print(profile_dictionary['results_page_url'])\n",
    "        print('---------------')\n",
    "#         print(profile)\n",
    "#         print('NEXT ITERATION')\n",
    "    df = pd.DataFrame(list_of_scrapped_profiles)\n",
    "    df['date_scraped'] = datetime.now(pytz.utc)\n",
    "    return pd.DataFrame(list_of_scrapped_profiles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INPUT - single url of region page of profes region_url\n",
    "    #EXAMPLE- ='https://www.tusclasesparticulares.com/profesores-ingles/bilbao?pagina=8'\n",
    "#RETURN - dict url to status, need to properly process this\n",
    "\n",
    "def get_profile_urls(region_url, html):\n",
    "    \n",
    "    #html = get(region_url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    #list of profile urls for give region page\n",
    "    landing_page_dict_list=[]\n",
    "    page_rank = 0\n",
    "    for element in soup.find_all('div', {'class' : re.compile(r'^trr aitem')}):\n",
    "        results_ad_status = ' '.join(element['class'])\n",
    "        if element.find('span', {'class' : 'spr_parrilla fotoparr'}) is None:\n",
    "            has_picture=True\n",
    "        else:\n",
    "            has_picture=False\n",
    "        for link in element.find_all('div', {'class' : 'rightcontent'}):\n",
    "            if link.a is not None:\n",
    "                base_url = 'https://www.tusclasesparticulares.com'\n",
    "                scrapped_url = link.a.get(\"href\")\n",
    "                profile_url = base_url + scrapped_url\n",
    "                break\n",
    "            else:\n",
    "                profile_url = 'EMPTY'\n",
    "                break\n",
    "        page_rank += 1\n",
    "        profe_dict = {\n",
    "            'profile_url' : profile_url,\n",
    "            'results_page_url' : region_url,\n",
    "            'page_rank' : page_rank,\n",
    "            'results_ad_status' : results_ad_status,\n",
    "            'has_picture' : has_picture,\n",
    "            'html' : html}\n",
    "        print('PROFILE DICT JUST SCRAPED FROM RESULTS PAGE:')\n",
    "        print(profe_dict)\n",
    "        landing_page_dict_list.append(profe_dict)\n",
    "    return landing_page_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract data from each profile URL\n",
    "import socket\n",
    "\n",
    "\n",
    "def url_open_for_profile_extraction(url, return_dict):\n",
    "    html= urlopen(url)\n",
    "    return_dict[html] = html\n",
    "\n",
    "def profile_extraction(profile_dictionary):\n",
    "    url = profile_dictionary['profile_url']    \n",
    "\n",
    "    try:\n",
    "\n",
    "        html = urlopen(url)\n",
    "    except (TimeoutError, socket.timeout) as e:\n",
    "        print('TimeoutError or socket.timeout')\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        alternative_scraping = False\n",
    "    except timeout:\n",
    "        print('timeout: The read operation timed out')\n",
    "        pdb.set_trace()\n",
    "    #TEACHER NAME and RATING COUNT\n",
    "    name_element= soup.find_all('div', {'id' : 'detsubheader'})\n",
    "    if len(name_element) == 0:\n",
    "        #use alternative scraping method for everything\n",
    "        alternative_scraping = True\n",
    "        #overwrite soup with get(url) method\n",
    "        html = get(url)\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        name_element= soup.find_all('div', {'id' : 'detsubheader'})\n",
    "    for item in name_element:\n",
    "        teacher_name = item.find('a', {'id' : 'alnkname'})\n",
    "        found_rating = item.find('span', {'itemprop' : 'ratingCount'})\n",
    "        #check for prescence of this tag, means they are an academy\n",
    "        if item.find('b', {'id' : 'bhome'}):\n",
    "            is_academy = True\n",
    "        else:\n",
    "            is_academy = False\n",
    "    if teacher_name is not None:\n",
    "        #means link was found as teachers name, grab name and note that they have a profile page\n",
    "        teacher_name = teacher_name.text.strip()\n",
    "        teacher_has_profile_page = True\n",
    "    else:\n",
    "        # for case where teacher name is NOT link (teacher has no profile)\n",
    "        name_element = soup.find('div', {'id' : 'detsubheader'})\n",
    "        try:\n",
    "            teacher_name = name_element.text.strip()\n",
    "        except AttributeError:\n",
    "            teacher_name = 'AttributeError: NoneType object has no attribute text'\n",
    "        teacher_has_profile_page = False\n",
    "    if found_rating is not None:\n",
    "        rating_count = found_rating.text\n",
    "    else:\n",
    "        rating_count=0\n",
    "\n",
    "    #these should not be needed....\n",
    "    try:\n",
    "       teacher_name\n",
    "    except NameError:\n",
    "        teacher_name = 'NameError'\n",
    "    try:\n",
    "        rating_count\n",
    "    except NameError:\n",
    "        rating_count = 'NameError'    \n",
    "    \n",
    "    #TITLE\n",
    "    if len(soup.find_all('div', {'class' : 'detinfotit'})) == 0:\n",
    "        ad_title = 'COULD NOT SCRAPE TITLE'\n",
    "        #add code to try alternative scraping method\n",
    "    else:\n",
    "        title_element = soup.find_all('div', {'class' : 'detinfotit'})[0]\n",
    "        ad_title = title_element.text.strip()\n",
    "    try:\n",
    "       ad_title\n",
    "    except NameError:\n",
    "        ad_title = 'EMPTY'\n",
    "    \n",
    "    #TEACHING SUBJECT\n",
    "    #example:  Inglés\n",
    "    for item in soup.find_all('p', {'id' : 'pClasesde'}):\n",
    "        teaching_subject = item.text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").split()[2]\n",
    "    try:\n",
    "       teaching_subject\n",
    "    except NameError:\n",
    "        teaching_subject = 'EMPTY'\n",
    "\n",
    "    #GEO LOCATION\n",
    "    for item in soup.find_all('p', {'id' : 'pProvincia'}):\n",
    "        province = ' '.join(item.text.split()[1:])\n",
    "    try:\n",
    "       province\n",
    "    except NameError:\n",
    "        province = 'EMPTY'\n",
    "\n",
    "    #CLASS LEVEL\n",
    "    for item in soup.find_all('div', {'id' : 'dvPara'}):\n",
    "        class_level_para = ' '.join(item.text.split()[3:]) \n",
    "    #check if variable was assigned, if it wasn't it's Null\n",
    "    try:\n",
    "       class_level_para\n",
    "    except NameError:\n",
    "        class_level_para = 'EMPTY'\n",
    "    #CLASS LEVEL SECOND METHOD\n",
    "    for item in soup.find_all('div', {'id' : 'dvNiveles'}):\n",
    "        class_level_niveles = ' '.join(item.text.split()[1:])\n",
    "    try:\n",
    "       class_level_niveles\n",
    "    except NameError:\n",
    "        class_level_niveles = 'EMPTY'\n",
    "\n",
    "    #METHOD\n",
    "    for item in soup.find_all('div', {'id' : 'dvMetodos'}):\n",
    "        method = item.text.split('\\r')[1].strip()\n",
    "    try:\n",
    "       method\n",
    "    except NameError:\n",
    "        method = 'EMPTY'\n",
    "\n",
    "    #PRICE\n",
    "    for item in soup.find_all('div', {'id' : 'dvPrecio'}):\n",
    "        price = item.text.replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "    try:\n",
    "       price\n",
    "    except NameError:\n",
    "        price = 'EMPTY'\n",
    "            \n",
    "    #DESCRIPTION\n",
    "    for item in soup.find_all('div', {'class' : 'detcntsection c5'}):  \n",
    "        description = item.text.strip()\n",
    "    try:\n",
    "       description\n",
    "    except NameError:\n",
    "        description = 'EMPTY'\n",
    "        \n",
    "    #PROFE AD STATUS - BASIC, VERIFIED, PLUS\n",
    "    element = soup.find('p', {'class' : 'mgbottom5 fs16 bold'})\n",
    "    if element is None:\n",
    "        profe_ad_status = 'EMPTY'\n",
    "    else:\n",
    "        profe_ad_status = element.text\n",
    "        \n",
    "    #CREATE DICT WITH ALL ENTRIES AND RETURN DICTIONAR\n",
    "    scraped_profile={\n",
    "        'url': url,\n",
    "        'ad_title' : ad_title,\n",
    "        'teaching_subject' : teaching_subject,\n",
    "        'province' : province,\n",
    "        'class_level_para' : class_level_para,\n",
    "        'class_level_niveles' : class_level_niveles,\n",
    "        'method' : method,\n",
    "        'price' : price,\n",
    "        'teacher_name' : teacher_name,\n",
    "        'rating_count' : rating_count,\n",
    "        'profe_ad_status' : profe_ad_status,\n",
    "        'description' : description,\n",
    "        #add in data passed from results page\n",
    "        'page_rank' : profile_dictionary['page_rank'],\n",
    "        'results_ad_status' : profile_dictionary['results_ad_status'],\n",
    "        'has_picture' : profile_dictionary['has_picture'],\n",
    "        'results_page_url' : profile_dictionary['results_page_url'],\n",
    "        'alternative_scraping' : alternative_scraping,\n",
    "        'teacher_has_profile_page' : teacher_has_profile_page,\n",
    "        'is_academy' : is_academy,\n",
    "        'html' : html\n",
    "       }\n",
    "    print('RETURNING PROFILE DATA')\n",
    "    return scraped_profile\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SCRAPING AUTONOMOUS CAPITALS ONE BY ONE\n",
    "import socket\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "region_lookup = {'soria' : 'https://www.tusclasesparticulares.com/profesores-ingles/soria.aspx',\n",
    "                 'teruel': 'https://www.tusclasesparticulares.com/profesores-ingles/Teruel.aspx',\n",
    "                 'segovia' : 'https://www.tusclasesparticulares.com/profesores-ingles/segovia.aspx',\n",
    "                 'huesca' : 'https://www.tusclasesparticulares.com/profesores-ingles/huesca.aspx',\n",
    "                 'cuenca' : 'https://www.tusclasesparticulares.com/profesores-ingles/cuenca.aspx',\n",
    "                 'avila' : 'https://www.tusclasesparticulares.com/profesores-ingles/avila.aspx',\n",
    "                 'merida' : 'https://www.tusclasesparticulares.com/profesores-ingles/Merida.aspx',\n",
    "                 'zamora' : 'https://www.tusclasesparticulares.com/profesores-ingles/zamora.aspx',\n",
    "\n",
    "                 'ciudad_real' : 'https://www.tusclasesparticulares.com/profesores-ingles/ciudad-real.aspx',\n",
    "                 'palencia' : 'https://www.tusclasesparticulares.com/profesores-ingles/palencia.aspx',\n",
    "                 'pontevedra' : 'https://www.tusclasesparticulares.com/profesores-ingles/pontevedra.aspx',\n",
    "                 \n",
    "                 'toledo' : 'https://www.tusclasesparticulares.com/profesores-ingles/toledo.aspx',\n",
    "                 'guadalajara' : 'https://www.tusclasesparticulares.com/profesores-ingles/guadalajara.aspx',\n",
    "                 'ceuta' : 'https://www.tusclasesparticulares.com/profesores-ingles/ceuta.aspx',#13\n",
    "                 \n",
    "                 'melilla' : 'https://www.tusclasesparticulares.com/profesores-ingles/Melilla.aspx',#14\n",
    "                 'caceres' : 'https://www.tusclasesparticulares.com/profesores-ingles/caceres.aspx',\n",
    "                 'santiago_de_compostela' : 'https://www.tusclasesparticulares.com/profesores-ingles/santiago-de-compostela.aspx', #16\n",
    "                 'lugo' : 'https://www.tusclasesparticulares.com/profesores-ingles/lugo.aspx',\n",
    "                 'gerona' : 'https://www.tusclasesparticulares.com/profesores-ingles/girona.aspx',\n",
    "                 'orense' : 'https://www.tusclasesparticulares.com/profesores-ingles/orense.aspx',\n",
    "                 'jaen' : 'https://www.tusclasesparticulares.com/profesores-ingles/jaen.aspx',\n",
    "                 \n",
    "                 'cadiz' : 'https://www.tusclasesparticulares.com/profesores-ingles/cadiz.aspx',#21\n",
    "                 'leon' : 'https://www.tusclasesparticulares.com/profesores-ingles/leon.aspx',\n",
    "                 'tarragona' : 'https://www.tusclasesparticulares.com/profesores-ingles/tarragona.aspx',\n",
    "                 'lerida' : 'https://www.tusclasesparticulares.com/profesores-ingles/lerida.aspx', #24\n",
    "                 'salamanca' : 'https://www.tusclasesparticulares.com/profesores-ingles/salamanca.aspx', #25\n",
    "                 'huelva' : 'https://www.tusclasesparticulares.com/profesores-ingles/huelva.aspx',\n",
    "                 \n",
    "                 'badajoz' : 'https://www.tusclasesparticulares.com/profesores-ingles/badajoz.aspx', #27\n",
    "                 'logrono' : 'https://www.tusclasesparticulares.com/profesores-ingles/logrono.aspx',\n",
    "                 'castellon_de_la_plana' : 'https://www.tusclasesparticulares.com/profesores-ingles/castellon-de-la-plana.aspx',\n",
    "                 \n",
    "                 'santander' : 'https://www.tusclasesparticulares.com/profesores-ingles/santander.aspx', #30\n",
    "                 'albacete' : 'https://www.tusclasesparticulares.com/profesores-ingles/albacete.aspx',#31\n",
    "                 'burgos' : 'https://www.tusclasesparticulares.com/profesores-ingles/burgos.aspx',\n",
    "                 \n",
    "                 'san_sebastian' : 'https://www.tusclasesparticulares.com/profesores-ingles/san-sebastian.aspx', #33\n",
    "                 'almeria' : 'https://www.tusclasesparticulares.com/profesores-ingles/almeria.aspx',\n",
    "                 'pamplona' : 'https://www.tusclasesparticulares.com/profesores-ingles/pamplona.aspx',\n",
    "                 \n",
    "                 'tenerife' : 'https://www.tusclasesparticulares.com/profesores-ingles/santa-cruz-de-tenerife.aspx', #36\n",
    "                 'oviedo' : 'https://www.tusclasesparticulares.com/profesores-ingles/oviedo.aspx',\n",
    "                 'granada' : 'https://www.tusclasesparticulares.com/profesores-ingles/granada.aspx', #38\n",
    "                 'la_coruna' : 'https://www.tusclasesparticulares.com/profesores-ingles/a-coruna.aspx', #39\n",
    "                 'vitoria' : 'https://www.tusclasesparticulares.com/profesores-ingles/vitoria.aspx',\n",
    "                 'valladolid' : 'https://www.tusclasesparticulares.com/profesores-ingles/valladolid.aspx',\n",
    "                 'cordoba' : 'https://www.tusclasesparticulares.com/profesores-ingles/cordoba.aspx', #42\n",
    "                 \n",
    "                 'alicante' : 'https://www.tusclasesparticulares.com/profesores-ingles/alicante.aspx', #43\n",
    "                 'bilbao' : 'https://www.tusclasesparticulares.com/profesores-ingles/bilbao.aspx',\n",
    "                 'las_palmas_de_gran_canaria' : 'https://www.tusclasesparticulares.com/profesores-ingles/las-palmas-de-gran-canaria.aspx',\n",
    "                 'palma' : 'https://www.tusclasesparticulares.com/profesores-ingles/palma-de-mallorca.aspx',\n",
    "                 'murcia' : 'https://www.tusclasesparticulares.com/profesores-ingles/murcia.aspx',\n",
    "                 \n",
    "                 'malaga' : 'https://www.tusclasesparticulares.com/profesores-ingles/malaga.aspx', #48\n",
    "                 'zaragoza' : 'https://www.tusclasesparticulares.com/profesores-ingles/zaragoza.aspx', #49 \n",
    "                 'sevilla' : 'https://www.tusclasesparticulares.com/profesores-ingles/sevilla.aspx',\n",
    "                 'valencia' : 'https://www.tusclasesparticulares.com/profesores-ingles/valencia.aspx', #51\n",
    "                 'barcelona' : 'https://www.tusclasesparticulares.com/profesores-ingles/barcelona.aspx',\n",
    "                 'madrid' : 'https://www.tusclasesparticulares.com/profesores-ingles/madrid.aspx                \n",
    "                }\n",
    "\n",
    "try:\n",
    "   df_created_list\n",
    "except NameError:\n",
    "    df_created_list = []\n",
    "\n",
    "end_loop = len(region_lookup) \n",
    "#specify region to start in: 0 = first in list (soria)\n",
    "region_start = 0 \n",
    "\n",
    "for i in range(region_start, end_loop):\n",
    "    print('i is: ' + str(i))\n",
    "\n",
    "    region_name = list(region_lookup.keys())[i]\n",
    "    base_region_url = region_lookup[region_name]\n",
    "    print('region_name is ' + region_name)\n",
    "    print('base_region_url is ' + base_region_url)\n",
    "    \n",
    "    results_page_profile_dicts = []\n",
    "    counter = 1\n",
    "    current_page = base_region_url\n",
    "\n",
    "    #infinite loop and break out once pagination link false\n",
    "    while True:\n",
    "        html = get(current_page)\n",
    "        if html.url == current_page:\n",
    "            #If so, continue, pagination exists.  \n",
    "            #CONTINUE WITH SCRAPING, \n",
    "            #dictionary with each profile and additional data from results page\n",
    "            results_page_profile_dicts.extend(get_profile_urls(current_page, html))\n",
    "            counter +=1\n",
    "            current_page = base_region_url + '?pagina=' + str(counter)\n",
    "        #If not, no more pagination exists\n",
    "        else:\n",
    "            break\n",
    "            #finish results page scraping\n",
    "\n",
    "    list_of_scrapped_profiles = []\n",
    "    bad_url_profiles = []\n",
    "    try: \n",
    "        for profile_dictionary in results_page_profile_dicts:\n",
    "            \n",
    "            print('scraping this profile URL:')\n",
    "            print(profile_dictionary['profile_url'])\n",
    "            print('from this url results page:')\n",
    "            print(profile_dictionary['results_page_url'])\n",
    "            print('---------------')\n",
    "\n",
    "\n",
    "            scrapped_profile = profile_extraction(profile_dictionary)\n",
    "            list_of_scrapped_profiles.append(scrapped_profile)\n",
    " \n",
    "    except (UnboundLocalError, KeyboardInterrupt) as e:\n",
    "        print('buttnast')\n",
    "        continue\n",
    "        pdb.set_trace()\n",
    "\n",
    "    #build_DF\n",
    "    temp_df = pd.DataFrame(list_of_scrapped_profiles)\n",
    "    temp_df['date_scraped'] = datetime.now(pytz.utc)\n",
    "    temp_df['base_region'] = region_name\n",
    "\n",
    "    vars()[region_name] = temp_df\n",
    "    print('df created for region of: ' + region_name)\n",
    "    df_created_list.append(region_name)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('elapsed time is:')\n",
    "    print(elapsed_time/60)\n",
    "\n",
    "print('  madrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "start scraping at:\n",
      "9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IF SCRAPING ISSUE, USE TO CONTINUE PROCESS AT GIVEN #\n",
    "\n",
    "print(len(results_page_profile_dicts))\n",
    "print('start scraping at:')\n",
    "print(len(list_of_scrapped_profiles))\n",
    "type(results_page_profile_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTINUE LOOP FROM MAIN SCRAPING PROCESS, based on N in previous cell\n",
    "\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "for profile_dictionary in results_page_profile_dicts[8374:]:\n",
    "\n",
    "            print('scraping this profile URL:')\n",
    "            print(profile_dictionary['profile_url'])\n",
    "            print('from this url results page:')\n",
    "            print(profile_dictionary['results_page_url'])\n",
    "            print('---------------')\n",
    "            try:\n",
    "                scrapped_profile = profile_extraction(profile_dictionary)\n",
    "            except (HTTPError, UnboundLocalError) as e:\n",
    "                continue\n",
    "            list_of_scrapped_profiles.append(scrapped_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df created for region of: madrid\n"
     ]
    }
   ],
   "source": [
    "#create df based on region name and write out data.  save created DF's to list. \n",
    "temp_df = pd.DataFrame(list_of_scrapped_profiles)\n",
    "temp_df['date_scraped'] = datetime.now(pytz.utc)\n",
    "temp_df['base_region'] = region_name\n",
    "\n",
    "vars()[region_name] = temp_df\n",
    "print('df created for region of: ' + region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_scrapped_profiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ae89d68a2b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_of_scrapped_profiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_scrapped_profiles' is not defined"
     ]
    }
   ],
   "source": [
    "list_of_scrapped_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#WRITE OUT REGIONAL DATAFRAMES to CSV\n",
    "\n",
    "by_region_dfs_concat = pd.concat([\n",
    "#these are saved as 'by_region_dfs.csv'\n",
    "#soria,\n",
    "# segovia,\n",
    "# huesca,\n",
    "# cuenca,\n",
    "# avila,\n",
    "# merida,\n",
    "# zamora,\n",
    "# ciudad_real, \n",
    "# palencia, \n",
    "# pontevedra,\n",
    "# toledo,\n",
    "# guadalajara,\n",
    "# ceuta,\n",
    "# melilla,\n",
    "# caceres,\n",
    "# santiago_de_compostela,\n",
    "# lugo,\n",
    "# gerona,\n",
    "# orense,\n",
    "# jaen,\n",
    "# cadiz,\n",
    "# leon,\n",
    "# tarragona\n",
    "# lerida,\n",
    "# salamanca,\n",
    "# huelva,\n",
    "# badajoz, logrono, castellon_de_la_plana,\n",
    "# santander,\n",
    "# albacete, burgos,\n",
    "# san_sebastian, almeria, pamplona,\n",
    "# tenerife, oviedo,\n",
    "# granada,\n",
    "# la_coruna, vitoria, valladolid,\n",
    "# cordoba,\n",
    "# alicante, bilbao, las_palmas_de_gran_canaria, palma, murcia,\n",
    "malaga,\n",
    "zaragoza,\n",
    "sevilla,\n",
    "valencia,\n",
    "barcelona,\n",
    "madrid\n",
    "\n",
    "])\n",
    "by_region_dfs_concat.to_csv('by_region_dfs_MALAGA_to_MADRID.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
